{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a way to reproduce the graphs used in our experiments in an interactive way.  First, we will get a gene-gene interaction graph.  Then, we will add the driver features from the `MERGE` algorithm as node features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " loading from cache file/homes/gws/ewein/gene-expression/data/graphs/stringdb_graph_all_edges.adjlist\n"
     ]
    }
   ],
   "source": [
    "from data.gene_graphs import StringDBGraph\n",
    "graph = StringDBGraph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MERGE Features\n",
    "\n",
    "The following cell takes our gene interaction graph from above, and adds the MERGE features from each gene to the gene's respective node in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "features_df = pd.read_csv(\"data/merge_features.csv\")\n",
    "features_df = features_df.drop('#SIGPVALS', axis=1)\n",
    "features_df = features_df.drop('SCORE', axis=1)\n",
    "features_df = features_df.set_index('GENE')\n",
    "features_df = features_df.rename(\n",
    "    columns={\"meEthylation\": \"Methylation\",\n",
    "             \"Genomic abnormalities (CNV)\": \"CNV\",\n",
    "             \"Expression hub\": \"Hubness\"})\n",
    "features_df = features_df[~features_df.index.duplicated(keep='first')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patient Gene + Response Data\n",
    "\n",
    "We want to use the data from our gene interaction graph to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "patient_data_dir = '/fdata/ohsu_data/'\n",
    "patient_response_data = pickle.load(open(patient_data_dir+'final_frame.p','rb'))\n",
    "\n",
    "patient_response_data = patient_response_data[['patient_id', 'new drug', \"IC50\"]]\n",
    "drug = \"Quizartinib\"\n",
    "drug_indices = patient_response_data[\"new drug\"] == drug\n",
    "\n",
    "# Some patients had multiple ex vivo measurements taken for a given drug.  We aggregate\n",
    "# these measurements by taking the mean IC50 for each patient + drug combo\n",
    "patient_response_data_for_drug = patient_response_data[patient_response_data[\"new drug\"] == drug]\n",
    "mean_drug_response = patient_response_data_for_drug.groupby(\"patient_id\", as_index=False)['IC50'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gene Expression Data\n",
    "\n",
    "Now we load the gene expression data for our patients from the cell above.  The original dataset repeats a given patient's measurements for each drug measurement taken.  We don't need the duplicates here, so we drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rna_seq = pickle.load(open(patient_data_dir+'/X_rna_seq.p','rb'))\n",
    "rna_seq = rna_seq.loc[:,~rna_seq.columns.duplicated()]\n",
    "\n",
    "# Having the patient id for each entry (as opposed to just the numerical index) makes it easier\n",
    "# to verify we're doing things correctly later on\n",
    "rna_seq['patient_id'] = patient_response_data['patient_id']\n",
    "rna_seq = rna_seq.drop_duplicates(subset='patient_id')\n",
    "rna_seq = rna_seq.sort_values(by='patient_id')\n",
    "rna_seq_for_drug = rna_seq[rna_seq.patient_id.isin(mean_drug_response.patient_id)]\n",
    "rna_seq_for_drug = rna_seq_for_drug.drop('patient_id', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subgraphing\n",
    "\n",
    "Our sequencing data contains only a subset of the genes that are represented in our gene interaction graphs.  As such, we modify our interaction graph only to contain the nodes represented by the genes in our sequencing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "genes = rna_seq_for_drug.columns\n",
    "overlapping_genes = list(set(genes).intersection(features_df.index))\n",
    "graph.nx_graph = graph.nx_graph.subgraph(overlapping_genes)\n",
    "\n",
    "# Reorder our dataframes so that their genes have the same order as the nodes in our\n",
    "# gene-gene interaction graph\n",
    "rna_seq_for_drug = rna_seq_for_drug[list(graph.nx_graph.nodes)]\n",
    "features_df = features_df.reindex(list(graph.nx_graph.nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the Dataset\n",
    "\n",
    "### Input Data\n",
    "\n",
    "Our input data $X \\in \\mathbb{R}^{N \\times G \\times D}$ is composed of $N$ patients, each with $G$ genes and $D$ features for each gene (one for expression level and one for each driver feature).  Here we merge the RNA expression data that we have with the driver feature data from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_patients = rna_seq_for_drug.shape[0]\n",
    "num_genes    = rna_seq_for_drug.shape[1]\n",
    "num_features = features_df.shape[1] + 1 # Merge features plus one for gene expression level\n",
    "\n",
    "X = np.ndarray(shape = (num_patients, num_genes, num_features), dtype = float)\n",
    "\n",
    "for patient_index in range(num_patients):\n",
    "    patient_rna_expression = rna_seq_for_drug.iloc[patient_index]\n",
    "    new_patient = pd.DataFrame(index=patient_rna_expression.index, data={\"Expression\": patient_rna_expression})\n",
    "    new_patient = pd.concat([new_patient, features_df], axis=1)\n",
    "    X[patient_index] = new_patient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the Normalized Edge Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "import torch\n",
    "\n",
    "adj = sp.coo_matrix(nx.to_numpy_matrix(graph.nx_graph), dtype=np.float32)\n",
    "\n",
    "\n",
    "adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "adj = normalize(adj + sp.eye(adj.shape[0]))\n",
    "adj = sparse_mx_to_torch_sparse_tensor(adj).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Data\n",
    "\n",
    "Our output data $y \\in \\mathbb{R}^{N}$ is composed of drug response values for each of our $N$ patients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = mean_drug_response[\"IC50\"].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "First we define the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import MSELoss\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import datetime\n",
    "import time\n",
    "from IPython.core.debugger import set_trace\n",
    "from models.models import GCN\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def train(model, num_epochs, X, Y, batch_size):\n",
    "    # First split to 60% train, 40% test\n",
    "    # Then further divide the original 40% test into 20% validation and 20% test\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, train_size = 0.8, random_state=42)\n",
    "    #x_valid, x_test, y_valid, y_test = train_test_split(x_test, y_test, train_size=0.5, random_state=42)\n",
    "    \n",
    "    \n",
    "    #x_valid = torch.FloatTensor(x_valid).cuda()\n",
    "    #y_valid = torch.FloatTensor(y_valid).cuda()\n",
    "     \n",
    "    x_test = torch.FloatTensor(x_test).cuda()\n",
    "    y_test = torch.FloatTensor(y_test).cuda()\n",
    "    \n",
    "    print(\"Beginning model training at {}\".format(datetime.datetime.now()))\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        for i in range(0, x_train.shape[0], batch_size):\n",
    "            x_batch = torch.FloatTensor(x_train[i:i+batch_size]).cuda()\n",
    "            y_batch = torch.FloatTensor(y_train[i:i+batch_size]).cuda()\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Convert batch_size x 1 tensor into 1d tensor of length batch_size\n",
    "            output = model(x_batch, adj).squeeze(1)\n",
    "            \n",
    "            loss_train = criterion(output, y_batch)\n",
    "            print(\"Epoch {} batch {}/{} train loss {}\".format(epoch, i+batch_size, x_train.shape[0], loss_train))\n",
    "            loss_train.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        model.eval()\n",
    "        output = model(x_test, adj).squeeze(1)\n",
    "        loss_test = criterion(output, y_test)\n",
    "        end_time = time.time()\n",
    "        epoch_time = end_time - start_time\n",
    "        \n",
    "        print(\"Epoch {} completed in {} secs with test loss {:.4f}\".format(epoch, epoch_time, loss_test.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn = GCN(\n",
    "    nfeat = X.shape[2],\n",
    "    nhid = 16,\n",
    "    nnodes = X.shape[1],\n",
    "    dropout = 0.5\n",
    ").cuda()\n",
    "\n",
    "criterion = MSELoss(reduction='mean')\n",
    "optimizer = optim.Adam(gcn.parameters(), lr=1e-3, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we train it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning model training at 2019-10-10 12:54:31.473020\n",
      "Epoch 0 batch 32/171 train loss 11.946531295776367\n",
      "Epoch 0 batch 64/171 train loss 171288.71875\n",
      "Epoch 0 batch 96/171 train loss 3455.5556640625\n",
      "Epoch 0 batch 128/171 train loss 1220.82421875\n",
      "Epoch 0 batch 160/171 train loss 1664.7615966796875\n",
      "Epoch 0 batch 192/171 train loss 1439.8741455078125\n",
      "Epoch 0 completed in 323.96815824508667 secs with test loss 1233.8590\n",
      "Epoch 1 batch 32/171 train loss 1201.655029296875\n",
      "Epoch 1 batch 64/171 train loss 886.487060546875\n",
      "Epoch 1 batch 96/171 train loss 391.1154479980469\n",
      "Epoch 1 batch 128/171 train loss 57.28619384765625\n",
      "Epoch 1 batch 160/171 train loss 42.492523193359375\n",
      "Epoch 1 batch 192/171 train loss 210.697265625\n",
      "Epoch 1 completed in 322.37809228897095 secs with test loss 213.9976\n",
      "Epoch 2 batch 32/171 train loss 245.49647521972656\n",
      "Epoch 2 batch 64/171 train loss 253.244384765625\n",
      "Epoch 2 batch 96/171 train loss 148.51190185546875\n",
      "Epoch 2 batch 128/171 train loss 131.68734741210938\n",
      "Epoch 2 batch 160/171 train loss 81.99359893798828\n",
      "Epoch 2 batch 192/171 train loss 48.045719146728516\n",
      "Epoch 2 completed in 322.33705139160156 secs with test loss 9.1967\n",
      "Epoch 3 batch 32/171 train loss 33.139923095703125\n",
      "Epoch 3 batch 64/171 train loss 28.148773193359375\n",
      "Epoch 3 batch 96/171 train loss 63.627315521240234\n",
      "Epoch 3 batch 128/171 train loss 54.150115966796875\n",
      "Epoch 3 batch 160/171 train loss 69.08656311035156\n",
      "Epoch 3 batch 192/171 train loss 33.73685073852539\n",
      "Epoch 3 completed in 321.36318278312683 secs with test loss 48.7147\n",
      "Epoch 4 batch 32/171 train loss 76.01423645019531\n",
      "Epoch 4 batch 64/171 train loss 75.28330993652344\n",
      "Epoch 4 batch 96/171 train loss 39.63724899291992\n",
      "Epoch 4 batch 128/171 train loss 27.867496490478516\n",
      "Epoch 4 batch 160/171 train loss 39.374053955078125\n",
      "Epoch 4 batch 192/171 train loss 31.153575897216797\n",
      "Epoch 4 completed in 321.7171185016632 secs with test loss 10.0928\n",
      "Epoch 5 batch 32/171 train loss 52.26695251464844\n",
      "Epoch 5 batch 64/171 train loss 33.50172424316406\n",
      "Epoch 5 batch 96/171 train loss 44.514495849609375\n",
      "Epoch 5 batch 128/171 train loss 27.973495483398438\n",
      "Epoch 5 batch 160/171 train loss 31.53691864013672\n",
      "Epoch 5 batch 192/171 train loss 35.85016632080078\n",
      "Epoch 5 completed in 302.11117148399353 secs with test loss 9.2075\n",
      "Epoch 6 batch 32/171 train loss 18.125185012817383\n",
      "Epoch 6 batch 64/171 train loss 23.091217041015625\n",
      "Epoch 6 batch 96/171 train loss 39.87112808227539\n",
      "Epoch 6 batch 128/171 train loss 19.253520965576172\n",
      "Epoch 6 batch 160/171 train loss 29.454553604125977\n",
      "Epoch 6 batch 192/171 train loss 22.135963439941406\n",
      "Epoch 6 completed in 322.28391289711 secs with test loss 14.7085\n",
      "Epoch 7 batch 32/171 train loss 25.037479400634766\n",
      "Epoch 7 batch 64/171 train loss 38.484580993652344\n",
      "Epoch 7 batch 96/171 train loss 22.288074493408203\n",
      "Epoch 7 batch 128/171 train loss 21.521549224853516\n",
      "Epoch 7 batch 160/171 train loss 23.052978515625\n",
      "Epoch 7 batch 192/171 train loss 16.7101993560791\n",
      "Epoch 7 completed in 322.3395206928253 secs with test loss 9.7923\n",
      "Epoch 8 batch 32/171 train loss 23.713666915893555\n",
      "Epoch 8 batch 64/171 train loss 13.104454040527344\n",
      "Epoch 8 batch 96/171 train loss 22.47842025756836\n",
      "Epoch 8 batch 128/171 train loss 17.548328399658203\n",
      "Epoch 8 batch 160/171 train loss 24.237228393554688\n",
      "Epoch 8 batch 192/171 train loss 20.090665817260742\n",
      "Epoch 8 completed in 321.8272931575775 secs with test loss 9.3369\n",
      "Epoch 9 batch 32/171 train loss 21.723806381225586\n",
      "Epoch 9 batch 64/171 train loss 31.893333435058594\n",
      "Epoch 9 batch 96/171 train loss 29.480195999145508\n",
      "Epoch 9 batch 128/171 train loss 14.795915603637695\n",
      "Epoch 9 batch 160/171 train loss 23.69870376586914\n",
      "Epoch 9 batch 192/171 train loss 15.703601837158203\n",
      "Epoch 9 completed in 321.3542654514313 secs with test loss 8.8932\n",
      "Epoch 10 batch 32/171 train loss 26.0143985748291\n",
      "Epoch 10 batch 64/171 train loss 40.85640335083008\n",
      "Epoch 10 batch 96/171 train loss 16.34969139099121\n",
      "Epoch 10 batch 128/171 train loss 19.85727310180664\n",
      "Epoch 10 batch 160/171 train loss 17.103994369506836\n",
      "Epoch 10 batch 192/171 train loss 19.057573318481445\n",
      "Epoch 10 completed in 323.0890893936157 secs with test loss 8.9593\n",
      "Epoch 11 batch 32/171 train loss 16.363855361938477\n",
      "Epoch 11 batch 64/171 train loss 24.73828887939453\n",
      "Epoch 11 batch 96/171 train loss 19.749542236328125\n",
      "Epoch 11 batch 128/171 train loss 15.873211860656738\n",
      "Epoch 11 batch 160/171 train loss 15.338390350341797\n",
      "Epoch 11 batch 192/171 train loss 5.508480072021484\n",
      "Epoch 11 completed in 323.41614818573 secs with test loss 9.5702\n",
      "Epoch 12 batch 32/171 train loss 17.52372932434082\n",
      "Epoch 12 batch 64/171 train loss 31.44928741455078\n",
      "Epoch 12 batch 96/171 train loss 20.64369773864746\n",
      "Epoch 12 batch 128/171 train loss 14.979390144348145\n",
      "Epoch 12 batch 160/171 train loss 14.207717895507812\n",
      "Epoch 12 batch 192/171 train loss 15.126323699951172\n",
      "Epoch 12 completed in 323.3910789489746 secs with test loss 9.1533\n",
      "Epoch 13 batch 32/171 train loss 12.126435279846191\n",
      "Epoch 13 batch 64/171 train loss 15.007913589477539\n",
      "Epoch 13 batch 96/171 train loss 19.522499084472656\n",
      "Epoch 13 batch 128/171 train loss 9.013568878173828\n",
      "Epoch 13 batch 160/171 train loss 15.150860786437988\n",
      "Epoch 13 batch 192/171 train loss 9.283007621765137\n",
      "Epoch 13 completed in 323.6250088214874 secs with test loss 8.9943\n",
      "Epoch 14 batch 32/171 train loss 15.28414535522461\n",
      "Epoch 14 batch 64/171 train loss 18.62894058227539\n",
      "Epoch 14 batch 96/171 train loss 12.57657527923584\n",
      "Epoch 14 batch 128/171 train loss 13.137516021728516\n",
      "Epoch 14 batch 160/171 train loss 12.37238597869873\n",
      "Epoch 14 batch 192/171 train loss 11.410314559936523\n",
      "Epoch 14 completed in 323.2217619419098 secs with test loss 9.2956\n",
      "Epoch 15 batch 32/171 train loss 11.233643531799316\n",
      "Epoch 15 batch 64/171 train loss 10.67674732208252\n",
      "Epoch 15 batch 96/171 train loss 14.361041069030762\n",
      "Epoch 15 batch 128/171 train loss 10.09346866607666\n",
      "Epoch 15 batch 160/171 train loss 16.127887725830078\n",
      "Epoch 15 batch 192/171 train loss 2.70317006111145\n",
      "Epoch 15 completed in 322.8976399898529 secs with test loss 9.6533\n",
      "Epoch 16 batch 32/171 train loss 18.25627326965332\n",
      "Epoch 16 batch 64/171 train loss 13.016519546508789\n",
      "Epoch 16 batch 96/171 train loss 22.462095260620117\n",
      "Epoch 16 batch 128/171 train loss 10.548135757446289\n",
      "Epoch 16 batch 160/171 train loss 11.98878288269043\n",
      "Epoch 16 batch 192/171 train loss 7.345123291015625\n",
      "Epoch 16 completed in 323.39906764030457 secs with test loss 8.9734\n",
      "Epoch 17 batch 32/171 train loss 14.634831428527832\n",
      "Epoch 17 batch 64/171 train loss 9.536955833435059\n",
      "Epoch 17 batch 96/171 train loss 13.668083190917969\n",
      "Epoch 17 batch 128/171 train loss 16.110687255859375\n",
      "Epoch 17 batch 160/171 train loss 18.127714157104492\n",
      "Epoch 17 batch 192/171 train loss 9.58007526397705\n",
      "Epoch 17 completed in 324.1692361831665 secs with test loss 8.8897\n",
      "Epoch 18 batch 32/171 train loss 12.659248352050781\n",
      "Epoch 18 batch 64/171 train loss 13.53656005859375\n",
      "Epoch 18 batch 96/171 train loss 18.48692512512207\n",
      "Epoch 18 batch 128/171 train loss 7.622358322143555\n",
      "Epoch 18 batch 160/171 train loss 14.42780590057373\n",
      "Epoch 18 batch 192/171 train loss 4.074699878692627\n",
      "Epoch 18 completed in 325.17946672439575 secs with test loss 9.1354\n",
      "Epoch 19 batch 32/171 train loss 9.291873931884766\n",
      "Epoch 19 batch 64/171 train loss 14.574495315551758\n",
      "Epoch 19 batch 96/171 train loss 20.58403778076172\n",
      "Epoch 19 batch 128/171 train loss 8.339859962463379\n",
      "Epoch 19 batch 160/171 train loss 13.735350608825684\n",
      "Epoch 19 batch 192/171 train loss 10.16201114654541\n",
      "Epoch 19 completed in 325.1215126514435 secs with test loss 8.9180\n",
      "Epoch 20 batch 32/171 train loss 8.451681137084961\n",
      "Epoch 20 batch 64/171 train loss 15.90868091583252\n",
      "Epoch 20 batch 96/171 train loss 18.1036319732666\n",
      "Epoch 20 batch 128/171 train loss 10.315011024475098\n",
      "Epoch 20 batch 160/171 train loss 12.473611831665039\n",
      "Epoch 20 batch 192/171 train loss 6.867175102233887\n",
      "Epoch 20 completed in 324.8974070549011 secs with test loss 8.8797\n",
      "Epoch 21 batch 32/171 train loss 13.238877296447754\n",
      "Epoch 21 batch 64/171 train loss 11.897335052490234\n",
      "Epoch 21 batch 96/171 train loss 15.811840057373047\n",
      "Epoch 21 batch 128/171 train loss 10.922752380371094\n",
      "Epoch 21 batch 160/171 train loss 14.092916488647461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 batch 192/171 train loss 3.9321792125701904\n",
      "Epoch 21 completed in 324.6832346916199 secs with test loss 8.9152\n",
      "Epoch 22 batch 32/171 train loss 9.954179763793945\n",
      "Epoch 22 batch 64/171 train loss 15.610614776611328\n",
      "Epoch 22 batch 96/171 train loss 17.423084259033203\n",
      "Epoch 22 batch 128/171 train loss 8.092924118041992\n",
      "Epoch 22 batch 160/171 train loss 9.562019348144531\n",
      "Epoch 22 batch 192/171 train loss 7.328801155090332\n",
      "Epoch 22 completed in 325.32474517822266 secs with test loss 8.8778\n",
      "Epoch 23 batch 32/171 train loss 9.76272201538086\n",
      "Epoch 23 batch 64/171 train loss 13.021763801574707\n",
      "Epoch 23 batch 96/171 train loss 19.52459716796875\n",
      "Epoch 23 batch 128/171 train loss 7.218841552734375\n",
      "Epoch 23 batch 160/171 train loss 8.734879493713379\n",
      "Epoch 23 batch 192/171 train loss 6.853228569030762\n",
      "Epoch 23 completed in 325.46864461898804 secs with test loss 8.9816\n",
      "Epoch 24 batch 32/171 train loss 9.775869369506836\n",
      "Epoch 24 batch 64/171 train loss 9.763372421264648\n",
      "Epoch 24 batch 96/171 train loss 12.707906723022461\n",
      "Epoch 24 batch 128/171 train loss 7.2356367111206055\n",
      "Epoch 24 batch 160/171 train loss 10.543235778808594\n",
      "Epoch 24 batch 192/171 train loss 3.715395450592041\n",
      "Epoch 24 completed in 325.4868075847626 secs with test loss 9.0014\n",
      "Epoch 25 batch 32/171 train loss 9.825801849365234\n",
      "Epoch 25 batch 64/171 train loss 11.333269119262695\n",
      "Epoch 25 batch 96/171 train loss 13.550873756408691\n",
      "Epoch 25 batch 128/171 train loss 6.730597496032715\n",
      "Epoch 25 batch 160/171 train loss 8.232809066772461\n",
      "Epoch 25 batch 192/171 train loss 4.158259391784668\n",
      "Epoch 25 completed in 322.2636706829071 secs with test loss 9.0060\n",
      "Epoch 26 batch 32/171 train loss 8.966550827026367\n",
      "Epoch 26 batch 64/171 train loss 9.909748077392578\n",
      "Epoch 26 batch 96/171 train loss 13.709185600280762\n",
      "Epoch 26 batch 128/171 train loss 5.481777191162109\n",
      "Epoch 26 batch 160/171 train loss 7.1403584480285645\n",
      "Epoch 26 batch 192/171 train loss 4.532505035400391\n",
      "Epoch 26 completed in 305.8906512260437 secs with test loss 8.8818\n",
      "Epoch 27 batch 32/171 train loss 9.43433666229248\n",
      "Epoch 27 batch 64/171 train loss 10.602608680725098\n",
      "Epoch 27 batch 96/171 train loss 12.881763458251953\n",
      "Epoch 27 batch 128/171 train loss 5.986503601074219\n",
      "Epoch 27 batch 160/171 train loss 8.38336181640625\n",
      "Epoch 27 batch 192/171 train loss 4.926808834075928\n",
      "Epoch 27 completed in 322.96621966362 secs with test loss 8.8852\n",
      "Epoch 28 batch 32/171 train loss 9.476360321044922\n",
      "Epoch 28 batch 64/171 train loss 12.621915817260742\n",
      "Epoch 28 batch 96/171 train loss 12.861040115356445\n"
     ]
    }
   ],
   "source": [
    "train(gcn, 100, X, Y, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP with Gene Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.models import MLP\n",
    "from torch.nn import MSELoss\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "X = rna_seq_for_drug.values\n",
    "Y = mean_drug_response[\"IC50\"].to_numpy()\n",
    "\n",
    "\n",
    "def train(model, num_epochs, X, Y, batch_size):\n",
    "    # First split to 60% train, 40% test\n",
    "    # Then further divide the original 40% test into 20% validation and 20% test\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, train_size = 0.8, random_state=42)\n",
    "    #x_valid, x_test, y_valid, y_test = train_test_split(x_test, y_test, train_size=0.5, random_state=42)\n",
    "    \n",
    "    \n",
    "    #x_valid = torch.FloatTensor(x_valid).cuda()\n",
    "    #y_valid = torch.FloatTensor(y_valid).cuda()\n",
    "     \n",
    "    x_test = torch.FloatTensor(x_test).cuda()\n",
    "    y_test = torch.FloatTensor(y_test).cuda()\n",
    "    \n",
    "    print(\"Beginning model training at {}\".format(datetime.datetime.now()))\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        for i in range(0, x_train.shape[0], batch_size):\n",
    "            x_batch = torch.FloatTensor(x_train[i:i+batch_size]).cuda()\n",
    "            y_batch = torch.FloatTensor(y_train[i:i+batch_size]).cuda()\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Convert batch_size x 1 tensor into 1d tensor of length batch_size\n",
    "            output = model(x_batch).squeeze(1)\n",
    "            \n",
    "            loss_train = criterion(output, y_batch)\n",
    "            print(\"Epoch {} batch {}/{} train loss {}\".format(epoch, i+batch_size, x_train.shape[0], loss_train))\n",
    "            loss_train.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        model.eval()\n",
    "        output = model(x_test).squeeze(1)\n",
    "        loss_test = criterion(output, y_test)\n",
    "        end_time = time.time()\n",
    "        epoch_time = end_time - start_time\n",
    "        \n",
    "        print(\"Epoch {} completed in {} secs with test loss {:.4f}\".format(epoch, epoch_time, loss_test.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP(\n",
    "    nnodes = X.shape[1],\n",
    "    dropout = 0.5\n",
    ").cuda()\n",
    "\n",
    "criterion = MSELoss(reduction='mean')\n",
    "optimizer = optim.Adam(mlp.parameters(),\n",
    "                       lr=1e-5, weight_decay=5e-4)\n",
    "\n",
    "train(mlp, 100, X, Y, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCN only with Gene Expression Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_patients = rna_seq_for_drug.shape[0]\n",
    "num_genes    = rna_seq_for_drug.shape[1]\n",
    "num_features = 1 # Only care about gene expression level for the baseline\n",
    "\n",
    "X = np.ndarray(shape = (num_patients, num_genes, num_features), dtype = float)\n",
    "\n",
    "for patient_index in range(num_patients):\n",
    "    patient_rna_expression = rna_seq_for_drug.iloc[patient_index]\n",
    "    new_patient = pd.DataFrame(index=patient_rna_expression.index, data={\"Expression\": patient_rna_expression})\n",
    "    X[patient_index] = new_patient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the baseline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.models import GCN\n",
    "\n",
    "gcn_expression_only = GCN(\n",
    "    nfeat = X.shape[2],\n",
    "    nhid = 16,\n",
    "    nnodes = X.shape[1],\n",
    "    dropout = 0.5\n",
    ").cuda()\n",
    "\n",
    "criterion = MSELoss(reduction='mean')\n",
    "optimizer = optim.Adam(gcn_expression_only.parameters(),\n",
    "                       lr=1e-5, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = mean_drug_response[\"IC50\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(gcn_expression_only, 100, X, Y, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ElasticNet Regression on Gene Expression Levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "num_patients = rna_seq_for_drug.shape[0]\n",
    "num_genes    = rna_seq_for_drug.shape[1]\n",
    "num_features = 1 # Only care about gene expression level for the baseline\n",
    "\n",
    "X = np.ndarray(shape = (num_patients, num_genes, num_features), dtype = float)\n",
    "\n",
    "for patient_index in range(num_patients):\n",
    "    patient_rna_expression = rna_seq_for_drug.iloc[patient_index]\n",
    "    new_patient = pd.DataFrame(index=patient_rna_expression.index, data={\"Expression\": patient_rna_expression})\n",
    "    X[patient_index] = new_patient\n",
    "    \n",
    "X = X.squeeze(2)\n",
    "y = patient_response_data_for_drug[\"IC50\"].to_numpy()\n",
    "\n",
    "train_valid_split=0.8\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(X, y, train_size=train_valid_split, test_size=1-train_valid_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_elasticnet_with_params(alpha, l1_ratio):\n",
    "    regr = ElasticNet(random_state=0, alpha = alpha, l1_ratio = l1_ratio)\n",
    "    regr.fit(x_train, y_train)\n",
    "    \n",
    "    y_pred = regr.predict(x_valid)\n",
    "    mse = np.mean(np.square(y_pred - y_valid))\n",
    "    print(\"Test error {:.4f} for alpha={:.4f} and l1_ratio={:.4f}\".format(mse, alpha, l1_ratio))\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "alpha_vals = np.linspace(start = 0, stop = 5, num = 20)\n",
    "l1_ratio = np.linspace(start = 0, stop = 1.0, num = 20)\n",
    "\n",
    "best_mse = float(\"inf\")\n",
    "for alpha, l1_ratio in itertools.product(alpha_vals, l1_ratio):\n",
    "    mse_new = train_elasticnet_with_params(alpha=alpha, l1_ratio=l1_ratio)\n",
    "    if mse_new < best_mse:\n",
    "        best_mse = mse_new\n",
    "\n",
    "print(\"Best elastic net MSE: {:.4f}\".format(best_mse))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
